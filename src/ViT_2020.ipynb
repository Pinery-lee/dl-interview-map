{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO2shUaqQwFJQmXbBClADOg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pinery-lee/dl-interview-map/blob/main/src/ViT_2020.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfDo1Jl3P85c",
        "outputId": "fd82b1cf-9ece-4a9f-e961-165597bec9cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vit_b_16结构： VisionTransformer(\n",
            "  (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
            "  (encoder): Encoder(\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "    (layers): Sequential(\n",
            "      (encoder_layer_0): EncoderBlock(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (self_attention): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): MLPBlock(\n",
            "          (layer0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (layer1): GELU(approximate='none')\n",
            "          (layer2): Dropout(p=0.0, inplace=False)\n",
            "          (layer3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (layer4): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (encoder_layer_1): EncoderBlock(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (self_attention): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): MLPBlock(\n",
            "          (layer0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (layer1): GELU(approximate='none')\n",
            "          (layer2): Dropout(p=0.0, inplace=False)\n",
            "          (layer3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (layer4): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (encoder_layer_2): EncoderBlock(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (self_attention): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): MLPBlock(\n",
            "          (layer0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (layer1): GELU(approximate='none')\n",
            "          (layer2): Dropout(p=0.0, inplace=False)\n",
            "          (layer3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (layer4): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (encoder_layer_3): EncoderBlock(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (self_attention): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): MLPBlock(\n",
            "          (layer0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (layer1): GELU(approximate='none')\n",
            "          (layer2): Dropout(p=0.0, inplace=False)\n",
            "          (layer3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (layer4): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (encoder_layer_4): EncoderBlock(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (self_attention): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): MLPBlock(\n",
            "          (layer0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (layer1): GELU(approximate='none')\n",
            "          (layer2): Dropout(p=0.0, inplace=False)\n",
            "          (layer3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (layer4): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (encoder_layer_5): EncoderBlock(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (self_attention): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): MLPBlock(\n",
            "          (layer0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (layer1): GELU(approximate='none')\n",
            "          (layer2): Dropout(p=0.0, inplace=False)\n",
            "          (layer3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (layer4): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (encoder_layer_6): EncoderBlock(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (self_attention): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): MLPBlock(\n",
            "          (layer0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (layer1): GELU(approximate='none')\n",
            "          (layer2): Dropout(p=0.0, inplace=False)\n",
            "          (layer3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (layer4): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (encoder_layer_7): EncoderBlock(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (self_attention): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): MLPBlock(\n",
            "          (layer0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (layer1): GELU(approximate='none')\n",
            "          (layer2): Dropout(p=0.0, inplace=False)\n",
            "          (layer3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (layer4): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (encoder_layer_8): EncoderBlock(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (self_attention): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): MLPBlock(\n",
            "          (layer0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (layer1): GELU(approximate='none')\n",
            "          (layer2): Dropout(p=0.0, inplace=False)\n",
            "          (layer3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (layer4): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (encoder_layer_9): EncoderBlock(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (self_attention): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): MLPBlock(\n",
            "          (layer0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (layer1): GELU(approximate='none')\n",
            "          (layer2): Dropout(p=0.0, inplace=False)\n",
            "          (layer3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (layer4): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (encoder_layer_10): EncoderBlock(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (self_attention): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): MLPBlock(\n",
            "          (layer0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (layer1): GELU(approximate='none')\n",
            "          (layer2): Dropout(p=0.0, inplace=False)\n",
            "          (layer3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (layer4): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (encoder_layer_11): EncoderBlock(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (self_attention): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): MLPBlock(\n",
            "          (layer0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (layer1): GELU(approximate='none')\n",
            "          (layer2): Dropout(p=0.0, inplace=False)\n",
            "          (layer3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (layer4): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "  )\n",
            "  (heads): Sequential(\n",
            "    (head): Linear(in_features=768, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "输入形状: torch.Size([1, 3, 224, 224])\n",
            "输出形状: torch.Size([1, 10])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from functools import partial\n",
        "from collections import OrderedDict\n",
        "from typing import Optional, Callable\n",
        "\n",
        "class MLPBlock(nn.Module):\n",
        "    \"\"\"Transformer MLP 块 (GELU 激活函数)\"\"\"\n",
        "    def __init__(self, in_dim: int, mlp_dim: int, dropout: float):\n",
        "        super().__init__()\n",
        "        self.layer0 = nn.Linear(in_dim, mlp_dim)\n",
        "        self.layer1 = nn.GELU()\n",
        "        self.layer2 = nn.Dropout(dropout)\n",
        "        self.layer3 = nn.Linear(mlp_dim, in_dim)\n",
        "        self.layer4 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layer4(self.layer3(self.layer2(self.layer1(self.layer0(x)))))\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "    \"\"\"Transformer 编码器层\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_heads: int,\n",
        "        hidden_dim: int,\n",
        "        mlp_dim: int,\n",
        "        dropout: float,\n",
        "        attention_dropout: float,\n",
        "        norm_layer: Callable[..., nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # Self Attention 部分\n",
        "        self.ln_1 = norm_layer(hidden_dim)\n",
        "        self.self_attention = nn.MultiheadAttention(\n",
        "            hidden_dim, num_heads, dropout=attention_dropout, batch_first=True\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # MLP 部分\n",
        "        self.ln_2 = norm_layer(hidden_dim)\n",
        "        self.mlp = MLPBlock(hidden_dim, mlp_dim, dropout)\n",
        "\n",
        "    def forward(self, input: torch.Tensor):\n",
        "        # 残差连接 1\n",
        "        x = self.ln_1(input)\n",
        "        x, _ = self.self_attention(x, x, x, need_weights=False)\n",
        "        x = self.dropout(x)\n",
        "        x = x + input\n",
        "\n",
        "        # 残差连接 2\n",
        "        y = self.ln_2(x)\n",
        "        y = self.mlp(y)\n",
        "        return x + y\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"Transformer 编码器堆栈\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        seq_length: int,\n",
        "        num_layers: int,\n",
        "        num_heads: int,\n",
        "        hidden_dim: int,\n",
        "        mlp_dim: int,\n",
        "        dropout: float,\n",
        "        attention_dropout: float,\n",
        "        norm_layer: Callable[..., nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.pos_embedding = nn.Parameter(torch.empty(1, seq_length, hidden_dim).normal_(std=0.02))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        layers = OrderedDict()\n",
        "        for i in range(num_layers):\n",
        "            layers[f\"encoder_layer_{i}\"] = EncoderBlock(\n",
        "                num_heads, hidden_dim, mlp_dim, dropout, attention_dropout, norm_layer\n",
        "            )\n",
        "        self.layers = nn.Sequential(layers)\n",
        "        self.ln = norm_layer(hidden_dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = x + self.pos_embedding\n",
        "        x = self.dropout(x)\n",
        "        x = self.layers(x)\n",
        "        return self.ln(x)\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    \"\"\"纯净版 ViT 架构\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_size: int = 224,\n",
        "        patch_size: int = 16,\n",
        "        num_layers: int = 12,\n",
        "        num_heads: int = 12,\n",
        "        hidden_dim: int = 768,\n",
        "        mlp_dim: int = 3072,\n",
        "        dropout: float = 0.0,\n",
        "        attention_dropout: float = 0.0,\n",
        "        num_classes: int = 1000,\n",
        "        representation_size: Optional[int] = None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.image_size = image_size\n",
        "        self.patch_size = patch_size\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Patch Embedding: 使用卷积实现分块投影\n",
        "        self.conv_proj = nn.Conv2d(\n",
        "            in_channels=3, out_channels=hidden_dim, kernel_size=patch_size, stride=patch_size\n",
        "        )\n",
        "\n",
        "        seq_length = (image_size // patch_size) ** 2\n",
        "\n",
        "        # CLS Token\n",
        "        self.class_token = nn.Parameter(torch.zeros(1, 1, hidden_dim))\n",
        "        seq_length += 1\n",
        "\n",
        "        self.encoder = Encoder(\n",
        "            seq_length, num_layers, num_heads, hidden_dim, mlp_dim, dropout, attention_dropout\n",
        "        )\n",
        "\n",
        "        # 分类头\n",
        "        head_layers = OrderedDict()\n",
        "        if representation_size is None:\n",
        "            head_layers[\"head\"] = nn.Linear(hidden_dim, num_classes)\n",
        "        else:\n",
        "            head_layers[\"pre_logits\"] = nn.Linear(hidden_dim, representation_size)\n",
        "            head_layers[\"act\"] = nn.Tanh()\n",
        "            head_layers[\"head\"] = nn.Linear(representation_size, num_classes)\n",
        "        self.heads = nn.Sequential(head_layers)\n",
        "\n",
        "    def _process_input(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        n, c, h, w = x.shape\n",
        "        # (n, c, h, w) -> (n, hidden_dim, n_h, n_w)\n",
        "        x = self.conv_proj(x)\n",
        "        # (n, hidden_dim, n_h, n_w) -> (n, hidden_dim, seq_len)\n",
        "        x = x.reshape(n, self.hidden_dim, -1)\n",
        "        # (n, hidden_dim, seq_len) -> (n, seq_len, hidden_dim)\n",
        "        x = x.permute(0, 2, 1)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = self._process_input(x)\n",
        "        n = x.shape[0]\n",
        "\n",
        "        # 拼接 CLS Token\n",
        "        batch_class_token = self.class_token.expand(n, -1, -1)\n",
        "        x = torch.cat([batch_class_token, x], dim=1)\n",
        "\n",
        "        # 进入 Transformer Encoder\n",
        "        x = self.encoder(x)\n",
        "\n",
        "        # 取第一个 token (CLS) 进行分类\n",
        "        x = x[:, 0]\n",
        "        x = self.heads(x)\n",
        "        return x\n",
        "\n",
        "def vit_b_16(num_classes: int = 1000, **kwargs):\n",
        "    \"\"\"构建 ViT-Base 16 模型\"\"\"\n",
        "    return VisionTransformer(\n",
        "        image_size=224,\n",
        "        patch_size=16,\n",
        "        num_layers=12,\n",
        "        num_heads=12,\n",
        "        hidden_dim=768,\n",
        "        mlp_dim=3072,\n",
        "        num_classes=num_classes,\n",
        "        **kwargs\n",
        "    )\n",
        "\n",
        "# --- 测试代码 ---\n",
        "if __name__ == \"__main__\":\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model = vit_b_16(num_classes=10).to(device)\n",
        "    print(\"vit_b_16结构：\", model)\n",
        "\n",
        "    # 模拟一张 224x224 的三通道图片\n",
        "    img = torch.randn(1, 3, 224, 224).to(device)\n",
        "    output = model(img)\n",
        "\n",
        "    print(f\"输入形状: {img.shape}\")\n",
        "    print(f\"输出形状: {output.shape}\")  # 应该是 [1, 10]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# 1. 加载 Hugging Face 的预处理器和模型\n",
        "model_name = \"google/vit-base-patch16-224-in21k\"\n",
        "processor = AutoImageProcessor.from_pretrained(model_name)\n",
        "model = AutoModelForImageClassification.from_pretrained(model_name)\n",
        "print(\"vit-base-patch16-224-in21k结构：\", model)\n",
        "\n",
        "# 2. 修改分类头以适配 37 个类别\n",
        "# MobileNetV2 在 transformers 中的分类层叫 classifier\n",
        "in_features = model.classifier.in_features\n",
        "model.classifier = nn.Linear(in_features, 37)\n",
        "model.to(device)\n",
        "\n",
        "# 3. 准备数据集\n",
        "# 注意：使用预处理器中的均值和标准差\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=processor.image_mean, std=processor.image_std)\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.OxfordIIITPet(\n",
        "    root='./data', split='trainval', download=True, transform=transform\n",
        ")\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "# 4. 优化器和损失函数\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# 5. 训练循环\n",
        "print(\"开始微调 vit-base-patch16-224-in21k...\")\n",
        "model.train()\n",
        "for epoch in range(5):\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # transformers 模型的 forward 返回的是一个 sequence 分类输出对象\n",
        "        outputs = model(inputs)\n",
        "        logits = outputs.logits # 获取逻辑输出\n",
        "\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch + 1} Average Loss: {running_loss / len(train_loader):.4f}')\n",
        "\n",
        "print(\"训练结束\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7O7kGj0Q8Yg",
        "outputId": "0e87fb71-53bc-4501-c6c5-be421d133ff9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vit-base-patch16-224-in21k结构： ViTForImageClassification(\n",
            "  (vit): ViTModel(\n",
            "    (embeddings): ViTEmbeddings(\n",
            "      (patch_embeddings): ViTPatchEmbeddings(\n",
            "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
            "      )\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (encoder): ViTEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-11): 12 x ViTLayer(\n",
            "          (attention): ViTAttention(\n",
            "            (attention): ViTSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (output): ViTSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): ViTIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): ViTOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "  )\n",
            "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
            ")\n",
            "开始微调 vit-base-patch16-224-in21k...\n",
            "Epoch 1 Average Loss: 3.3891\n",
            "Epoch 2 Average Loss: 2.7971\n",
            "Epoch 3 Average Loss: 2.2463\n",
            "Epoch 4 Average Loss: 1.8026\n",
            "Epoch 5 Average Loss: 1.4454\n",
            "训练结束\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 测试\n",
        "# 加载测试集 (split='test')\n",
        "test_dataset = torchvision.datasets.OxfordIIITPet(\n",
        "    root='./data', split='test', download=True, transform=transform\n",
        ")\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "def evaluate_accuracy(model, data_loader, device):\n",
        "    model.eval()  # 切换到评估模式（关闭 Dropout）\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # 测试时不需要计算梯度，节省显存和计算资源\n",
        "    with torch.no_grad():\n",
        "        for data in data_loader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            outputs = model(images)\n",
        "\n",
        "            # 取概率最大的类别作为预测结果\n",
        "            _, predicted = torch.max(outputs.logits, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'模型在测试集上的准确率: {accuracy:.2f}%')\n",
        "    return accuracy\n",
        "\n",
        "# 训练结束后调用\n",
        "evaluate_accuracy(model, test_loader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJAhSmKHT_SP",
        "outputId": "0c6a69d6-54bf-427b-aa02-8311090d978f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "模型在测试集上的准确率: 91.17%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "91.16925592804579"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXzz7G1qUK0o",
        "outputId": "d1103b23-15dd-4db7-8506-6fb39c9094ec"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 看一下在Imagenet中，AlexNet的张量变化\n",
        "from torchinfo import summary\n",
        "summary(model, input_size=(1,3,224,224))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7p-N62dUV8v",
        "outputId": "0e7e4115-e48d-4300-82dd-c084e135d42c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "=========================================================================================================\n",
              "Layer (type:depth-idx)                                  Output Shape              Param #\n",
              "=========================================================================================================\n",
              "ViTForImageClassification                               [1, 37]                   --\n",
              "├─ViTModel: 1-1                                         [1, 197, 768]             --\n",
              "│    └─ViTEmbeddings: 2-1                               [1, 197, 768]             152,064\n",
              "│    │    └─ViTPatchEmbeddings: 3-1                     [1, 196, 768]             590,592\n",
              "│    │    └─Dropout: 3-2                                [1, 197, 768]             --\n",
              "│    └─ViTEncoder: 2-2                                  [1, 197, 768]             --\n",
              "│    │    └─ModuleList: 3-3                             --                        85,054,464\n",
              "│    └─LayerNorm: 2-3                                   [1, 197, 768]             1,536\n",
              "├─Linear: 1-2                                           [1, 37]                   28,453\n",
              "=========================================================================================================\n",
              "Total params: 85,827,109\n",
              "Trainable params: 85,827,109\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.MEGABYTES): 200.84\n",
              "=========================================================================================================\n",
              "Input size (MB): 0.60\n",
              "Forward/backward pass size (MB): 162.18\n",
              "Params size (MB): 342.70\n",
              "Estimated Total Size (MB): 505.49\n",
              "========================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    }
  ]
}